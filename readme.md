# ğŸ§  PyTorch-from-Scratch

**PyTorch-from-Scratch** is a minimal deep learning framework built from the ground up to **demystify the internals** of modern machine learning libraries like PyTorch and TensorFlow.  
It serves as a learning tool, a research sandbox, and a hackable base for experiments.

---

## ğŸš€ Project Vision

- **Educational First**: Learn how tensor operations, autograd, and neural networks work under the hood  
- **Hackable and Modular**: Easily extend core functionality in Python or C++  
- **Lightweight by Design**: Minimal dependencies, designed to run on CPU with optional CUDA support

---

## ğŸ§± Core Components

### 1. Tensor System
Multi-dimensional array library with core tensor operations and shape handling.

- Memory management with support for CPU (GPU support via CUDA planned)
- Math ops: `add`, `mul`, `matmul`, etc.
- Broadcasting and shape inference

### 2. Autograd Engine
Dynamic computation graph with automatic differentiation.

- Forward graph construction on-the-fly
- Reverse-mode autodiff (`.backward()`)
- Custom backward functions supported

### 3. Python API
Python-first design that mimics PyTorch's style for user familiarity.

- Operator overloading (e.g., `a + b`)
- Interoperable with NumPy
- Clean class-based architecture

### 4. Neural Network Module
Composable building blocks for model construction.

- Layers like `Linear`, `Conv2D`, `ReLU`
- Parameter tracking and weight initialization
- Module nesting and hooks

### 5. Optimization System
Standard training optimizers for parameter updates.

- Support for `SGD`, `Adam`, etc.
- Learning rate scheduling
- Momentum and weight decay

---

## ğŸ”§ Architecture Flow

```python
x = Tensor(...)  
y = model(x)  
loss = y.sum()  
loss.backward()  
```

Under the hood:

- Python objects map to efficient C++ Tensor structures  
- Operations build a computation graph dynamically  
- Gradients flow back automatically through the graph  
- Optimizer steps update weights  

---

## âš™ï¸ Technical Architecture

### Project Layers

```
[ Python API ]
     â†“
[ pybind11 Bindings ]
     â†“
[ C++ Core Library ]
```

### Build Flow

- Core C++: `libpytorch_core.so` (tensor ops, autograd)  
- Python Bindings: via **pybind11**  
- Python Package: high-level modules (`nn`, `optim`, etc.)

---

## ğŸ“¦ Project Structure

```
nn-from-scratch/
â”œâ”€â”€ cmake/                     # CMake configuration files
â”œâ”€â”€ include/                   # Public C++ headers
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ tensor/           # Tensor core implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ tensor.h
â”‚   â”‚   â”‚   â”œâ”€â”€ tensor_impl.h
â”‚   â”‚   â”‚   â””â”€â”€ tensor_ops.h
â”‚   â”‚   â”œâ”€â”€ autograd/         # Autograd components
â”‚   â”‚   â”‚   â”œâ”€â”€ function.h
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.h
â”‚   â”‚   â”‚   â””â”€â”€ variable.h
â”‚   â”‚   â”œâ”€â”€ nn/               # Neural network components
â”‚   â”‚   â”‚   â”œâ”€â”€ modules.h
â”‚   â”‚   â”‚   â””â”€â”€ functional.h
â”‚   â”‚   â””â”€â”€ utils/            # Utilities
â”‚   â”‚       â”œâ”€â”€ logging.h
â”‚   â”‚       â””â”€â”€ serializer.h
â”œâ”€â”€ src/                       # C++ implementation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ tensor/
â”‚   â”‚   â”‚   â”œâ”€â”€ tensor.cpp
â”‚   â”‚   â”‚   â””â”€â”€ tensor_ops.cpp
â”‚   â”‚   â”œâ”€â”€ autograd/
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.cpp
â”‚   â”‚   â”‚   â””â”€â”€ function.cpp
â”‚   â”‚   â””â”€â”€ nn/
â”‚   â”‚       â””â”€â”€ modules.cpp
â”‚   â””â”€â”€ python/               # Python binding sources
â”‚       â”œâ”€â”€ module.cpp
â”‚       â””â”€â”€ tensor.cpp
â”œâ”€â”€ python/                    # Python package
â”‚   â”œâ”€â”€ torchscratch/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tensor.py         # Python tensor class
â”‚   â”‚   â”œâ”€â”€ nn/               # Neural network modules
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ linear.py
â”‚   â”‚   â”‚   â””â”€â”€ functional.py
â”‚   â”‚   â”œâ”€â”€ optim/            # Optimizers
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ sgd.py
â”‚   â”‚   â”œâ”€â”€ utils/            # Python utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ data.py
â”‚   â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ csrc/             # Compiled extensions
â”œâ”€â”€ test/                      # Comprehensive tests
â”‚   â”œâ”€â”€ cpp/                  # C++ tests
â”‚   â”‚   â”œâ”€â”€ tensor/
â”‚   â”‚   â”‚   â””â”€â”€ test_tensor.cpp
â”‚   â”‚   â””â”€â”€ autograd/
â”‚   â”‚       â””â”€â”€ test_autograd.cpp
â”‚   â””â”€â”€ python/               # Python tests
â”‚       â”œâ”€â”€ test_tensor.py
â”‚       â””â”€â”€ test_nn.py
â”œâ”€â”€ third_party/              # External dependencies
â”‚   â”œâ”€â”€ pybind11/             # Python binding library
â”‚   â””â”€â”€ googletest/           # Google Test framework
â”œâ”€â”€ examples/                 # Example projects
â”‚   â”œâ”€â”€ mnist/
â”‚   â””â”€â”€ cifar10/
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ source/
â”‚   â”‚   â”œâ”€â”€ getting_started.rst
â”‚   â”‚   â””â”€â”€ api_reference.rst
â”‚   â””â”€â”€ Makefile
â”œâ”€â”€ scripts/                  # Maintenance scripts
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ format_code.sh
â”‚   â””â”€â”€ run_tests.sh
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/           # CI/CD pipelines
â”‚       â”œâ”€â”€ build.yml
â”‚       â””â”€â”€ test.yml
â”œâ”€â”€ CMakeLists.txt           # Root CMake configuration
â”œâ”€â”€ setup.py                 # Python package installation
â”œâ”€â”€ pyproject.toml          # Modern Python packaging config
â”œâ”€â”€ LICENSE
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸš° Key Challenges & Goals

| Area         | Challenge                           | Solution Goal                                |
|--------------|-------------------------------------|----------------------------------------------|
| Memory       | Tensor allocation & reuse           | Smart allocator & GPU memory hooks (CUDA)    |
| Performance  | Python-C++ overhead                 | Minimize bindings + vectorized kernels       |
| Autograd     | In-place ops, non-diff ops          | Robust backward graph & error handling       |
| API Design   | PyTorch-like UX vs complexity       | Intuitive syntax + clean abstraction layers  |

---

## ğŸŒ Future Plans

- âœ… CPU-first stable backend  
- ğŸ› ï¸ CUDA integration (GPU acceleration)  
- ğŸ”„ ONNX export & model serialization  
- ğŸŒ Distributed training (multi-GPU, autograd support)  
- ğŸ“¦ Plugin system for custom ops/layers/optimizers  

---

## ğŸ“š Why This Project Matters

| Role         | Value                                                              |
|--------------|--------------------------------------------------------------------|
| Learners     | Understand *how* PyTorch works under the hood                     |
| Researchers  | Safely prototype new models, backprop ideas, and layers           |
| Developers   | Contribute to a clean, modular, and understandable framework      |

---

## âœ… What Success Looks Like

Users can:

- Define custom neural networks  
- Train with `.backward()` + optimizers  
- Extend components in both C++ and Python  

The codebase stays:

- ğŸ“¦ **Lightweight** â€“ minimal setup  
- ğŸ§º **Testable** â€“ isolated units + integration tests  
- ğŸ’¡ **Hackable** â€“ core logic easy to follow and modify

---

## ğŸ¤ Contributing

Pull requests are welcome! We aim for clean code, thoughtful abstractions, and a welcoming space for learning and experimentation.

---

## ğŸ“„ License

MIT License â€“ Free to use, learn from, and build on.

